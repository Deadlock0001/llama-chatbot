# ğŸš€ LLaMA 3.1 GPU Chatbot (Docker + Streamlit + FastAPI)

A **production-style chatbot** powered by **LLaMA 3.1**, running fully on **GPU**, with a clean separation of concerns.

### Tech Stack
- ğŸ§  **Ollama** (LLaMA 3.1 on GPU)
- âš™ï¸ **FastAPI** (Backend API)
- ğŸ¨ **Streamlit** (Frontend UI)
- ğŸ³ **Docker & Docker Compose**

---

## ğŸ—ï¸ Architecture

Browser  
â†“  
Streamlit Frontend  
â†“ (HTTP)  
FastAPI Backend  
â†“  
Ollama (LLaMA 3.1 on GPU)

---

## ğŸ“ Project Structure

```
llama-chatbot/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## âœ… Prerequisites

### Hardware
- NVIDIA GPU (RTX / GTX)
- 8 GB VRAM minimum (RTX 3060 / 4060 recommended)

### Software
- Windows 10/11
- Docker Desktop
- WSL 2 enabled
- NVIDIA GPU drivers

Verify GPU:
```powershell
nvidia-smi
```

Verify Docker:
```powershell
docker --version
```

---

## ğŸ§ Enable WSL 2 (One-Time Setup)

Open **PowerShell as Administrator**:

```powershell
wsl --install
```

Restart your PC.

Verify:
```powershell
wsl --status
```

---

## â–¶ï¸ How to Start the Chatbot

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/llama-chatbot.git
cd llama-chatbot
```

---

### 2ï¸âƒ£ Start All Services (Docker)

```powershell
docker compose up --build
```

â³ First run may take several minutes.

---

### 3ï¸âƒ£ Download LLaMA 3.1 Model (One Time)

Open a **new terminal**:

```powershell
docker exec -it llama-chatbot-ollama-1 ollama pull llama3.1
```

---

### 4ï¸âƒ£ Open the Chatbot UI

Open your browser:

ğŸ‘‰ http://localhost:8501

You can now chat with LLaMA 3.1 ğŸ‰

---

## ğŸ§ª Verify GPU Usage

### System-wide GPU usage
```powershell
nvidia-smi -l 1
```

### GPU usage inside Ollama container
```powershell
docker exec -it llama-chatbot-ollama-1 nvidia-smi
```

### Ollama logs (definitive proof)
```powershell
docker logs llama-chatbot-ollama-1
```

Look for lines like:
```
using CUDA
offloaded 33/33 layers to GPU
```

---

## âš¡ Performance Notes

| Mode | Typical Response Time |
|----|------------------------|
| CPU | 20â€“60 seconds |
| GPU | 0.5â€“3 seconds |

The **first request** is slower due to model loading.

---

## ğŸ› ï¸ Stop the Server

```powershell
docker compose down
```

---

## ğŸ”§ GPU Configuration (Already Enabled)

```yaml
environment:
  - OLLAMA_NUM_GPU=1
  - OLLAMA_FLASH_ATTENTION=1
```

---

## ğŸš€ Future Improvements

- Token-by-token streaming
- Authentication (JWT)
- Persistent chat history (DB)
- Multi-user support
- Cloud GPU deployment

---

## ğŸ“œ License

MIT License

---

## â­ Support

If this project helped you, consider giving it a â­ on GitHub!
